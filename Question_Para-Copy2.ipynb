{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Query to Document and to Paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# --- Importing Various packages ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.stats as scipy\n",
    "\n",
    "# Tokenizers\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# Stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- GENSIM PACKAGE ---\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, doc2vec, Doc2Vec\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.summarization.bm25 import BM25\n",
    "\n",
    "### Keras import\n",
    "from keras.models import load_model\n",
    "from keras.layers import Activation\n",
    "\n",
    "# importing tensorflow for custom activation function\n",
    "import tensorflow as tf\n",
    "from keras.utils.generic_utils import get_custom_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Custom Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def custom_activation(x):\n",
    "    y = tf.clip_by_value(x,0,1,name=None)\n",
    "    return y\n",
    "get_custom_objects().update({'custom_activation': Activation(custom_activation)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "data_train = pd.read_json('data/squad_train_doc.json')\n",
    "data_train.rename(columns={'passages': 'documents'}, inplace=True)\n",
    "\n",
    "# # Contains the list all titles\n",
    "title_list = np.load('title_list.npy').tolist()\n",
    "\n",
    "# Contains the dictionary of title to context\n",
    "dictionary_document_context = np.load('dictionary_document_context.npy').item()\n",
    "\n",
    "tokenized_context_and_questions = np.load('tokenized_context_and_questions.npy').tolist()\n",
    "\n",
    "untokenized_context_and_questions = np.load('untokenized_context_and_questions.npy').tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Loading Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# BM25 MODEL\n",
    "BM_25_model = BM25(tokenized_context_and_questions)\n",
    "\n",
    "# TFIDF MODEL\n",
    "dictionary = corpora.Dictionary.load('model_data/squad.dict')\n",
    "corpus = corpora.MmCorpus('model_data/squad.mm')\n",
    "TFIDF_model = gensim.models.TfidfModel.load('model_data/TFIDF_model.bin')\n",
    "\n",
    "# Doc2Vec Model\n",
    "Doc2Vec_model = gensim.models.Doc2Vec.load('model_data/Doc2Vec_model.bin')\n",
    "\n",
    "# WMD model \n",
    "WMD_model = KeyedVectors.load('model_data/WMD_model.bin')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# DNN Perceptron model\n",
    "model_perceptron = load_model('model_data/final_dnn_perceptron_model.h5',custom_objects={'custom_activation': custom_activation})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## QUERY Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def BM25(query):    \n",
    "    ''' Accepts a question(query) to implement BM 25 Model.\n",
    "        Takes a query and word tokenizes it. \n",
    "              'get_scores' - Calculates the similarity distance between the tokenized-query and the document.\n",
    "\n",
    "        --> Returns a dataframe with Document name, Score and Rank\n",
    "    '''\n",
    "    scores = BM_25_model.get_scores(query.split(),1)\n",
    "    BM25_dataframe = pd.DataFrame({'Document':data_train.title, 'Score_BM25':scores}).sort_values(by=['Score_BM25'],ascending=False)\n",
    "    BM25_dataframe['Rank_BM25'] = [i for i in range(1, len(data_train.title)+1)]\n",
    "    return BM25_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def TFIDF(query): \n",
    "    ''' Accepts a question(query) to implement TF-IDF Model.\n",
    "        Takes a query and word tokenizes it. \n",
    "        'raw_corpus_query' - The word-tokenized query is compared with the dictionary used to train the document. \n",
    "            'corpus_query' - The word-id and word is converted into a corpus.The corpus is then fed to the TF-IDF model.\n",
    "        'similarity_table' - Stores the TF-IDF weights which are then used to get most similiar documents.\n",
    "                   'ranks' - Scipy method which compares the similarity weights and sorts is accordingly.\n",
    "\n",
    "        --> Returns a dataframe with Document name, Score and Rank\n",
    "    '''\n",
    "    query_1 = []\n",
    "    query_1.append(word_tokenize(query))\n",
    "    raw_corpus_query = [dictionary.doc2bow(word) for word in query_1]\n",
    "    corpora.MmCorpus.serialize('model_data/query3.mm',raw_corpus_query)\n",
    "    corpus_query = corpora.MmCorpus('model_data/query3.mm')\n",
    "    \n",
    "    similarity_table = TFIDF_model[corpus_query]\n",
    "    ranks = scipy.rankdata(similarity_table, method = 'max')\n",
    "    similarity_table = list(np.array(similarity_table).flatten())\n",
    "    TFIDF_dataframe = pd.DataFrame({'Document':data_train.title, 'Score_TFIDF':similarity_table}).sort_values(by=['Score_TFIDF'],ascending=False)\n",
    "    TFIDF_dataframe['Rank_TFIDF'] = [i for i in range(1, len(data_train.title)+1)]\n",
    "    return TFIDF_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def Doc2Vec(query):\n",
    "    ''' Accepts a question(query) to implement Doc2Vec Model.\n",
    "        Takes a query and word tokenizes it. \n",
    "           'avg_sentence' - After that the average of the sentenced words are compared with every document.\n",
    "           'most_similar' - Calculates the similarity distance between the avg of tokenized-sentence with every \n",
    "                            document iteratively.\n",
    "        'list_doc_scores' - Returns the sorted list of comparison with each doc in ascending order.\n",
    "\n",
    "        --> Returns a dataframe with Document name, Score and Rank(top_n, ascending order sorted)\n",
    "    '''\n",
    "\n",
    "    similarity_score_matrix , list_doc_names, list_doc_scores, list_doc_ranks, rank = [], [], [], [], 1\n",
    "    avg_sentence = np.zeros((200))\n",
    "    count = 0\n",
    "    for word in word_tokenize(query):\n",
    "        if word in Doc2Vec_model.wv.vocab:\n",
    "            avg_sentence +=  Doc2Vec_model[word]\n",
    "            count+=1\n",
    "    if count != 0:\n",
    "        avg_sentence = avg_sentence / count\n",
    "    similarity_score_matrix.append(Doc2Vec_model.docvecs.most_similar([avg_sentence], topn=len(title_list)))\n",
    "    for each_compared_row in similarity_score_matrix[0]:\n",
    "        list_doc_names.append(each_compared_row[0])\n",
    "        list_doc_scores.append(each_compared_row[1])\n",
    "        list_doc_ranks.append(rank)\n",
    "        rank += 1\n",
    "    query_comparison_dataframe = pd.DataFrame({'Document':list_doc_names, 'Score_Doc2Vec':list_doc_scores, 'Rank_Doc2Vec':list_doc_ranks})\n",
    "    return query_comparison_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Query to Document Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def query_to_document(query):\n",
    "    \"\"\" Takes string question and returns the name of the document which the question is likely to be present in\"\"\"\n",
    "    \n",
    "    bm25_df = BM25(query).head(n=50)         # gets the dataframe of BM25 with scores and ranks of documents\n",
    "    tfidf_df = TFIDF(query).head(n=50)       # gets the dataframe of TFIDF with scores and ranks of documents\n",
    "    doc2vec_df = Doc2Vec(query).head(n=50)   # gets the dataframe of Doc2Vec with scores and ranks of documents\n",
    "    \n",
    "    # combining all the dataframes\n",
    "    final_df = pd.merge(pd.merge(bm25_df,tfidf_df, on=['Document'], how='outer'), doc2vec_df, on=['Document'], how='outer')\n",
    "    final_df.fillna(0, inplace=True)\n",
    "    \n",
    "    # Normalising the scores between 0 and 1\n",
    "    bm25_normalised = (final_df.Score_BM25 - final_df.Score_BM25.min())/(final_df.Score_BM25.max()- final_df.Score_BM25.min())\n",
    "    tfidf_normalised = (final_df.Score_TFIDF-final_df.Score_TFIDF.min())/(final_df.Score_TFIDF.max()-final_df.Score_TFIDF.min())\n",
    "    doc2vec_normalised = (final_df.Score_Doc2Vec-final_df.Score_Doc2Vec.min())/(final_df.Score_Doc2Vec.max()-final_df.Score_Doc2Vec.min())\n",
    "    \n",
    "    # Getting the total score based on the preious overall accuracy\n",
    "    final_df['total_score'] = 0.01243557 * bm25_normalised + 0.29682442 * tfidf_normalised - 0.01673123 * doc2vec_normalised\n",
    "    \n",
    "    final_df['bm25_normalised'] = bm25_normalised\n",
    "    final_df['tfidf_normalised'] = tfidf_normalised\n",
    "    final_df['doc2vec_normalised'] = doc2vec_normalised\n",
    "    \n",
    "    final_document_list = final_df.Document.values[:]     \n",
    "    final_scores = np.array(final_df.loc[:,['bm25_normalised', 'tfidf_normalised','doc2vec_normalised']])\n",
    "    \n",
    "    prediction_scores = []\n",
    "    for document, scores in zip(final_document_list, final_scores):\n",
    "        scores = np.array(scores).reshape(1,3)\n",
    "        prediction = model_perceptron.predict(scores)\n",
    "        prediction_scores.append(prediction)\n",
    "    return final_document_list[np.array(prediction_scores).argmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Document to Paragraph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def document_to_paragraph(query, document):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    sent1 = [word for word in word_tokenize(query) if word not in stop_words]\n",
    "    tag = nltk.pos_tag(sent1)\n",
    "    words = []\n",
    "    for each_tag in tag:\n",
    "        if each_tag[1] == 'NN' or each_tag[1] == 'NNP' or each_tag[1] == 'NNS' or each_tag[1] == 'VBD' or each_tag[1] == 'VB':\n",
    "            words.append(each_tag[0])\n",
    "    sent1 = words\n",
    "    index = 0\n",
    "    sentences = sent_tokenize(document)\n",
    "    list_distances, list_sentence_index = [], []\n",
    "    for each_sentence in sentences:\n",
    "        sent2 = [word for word in word_tokenize(each_sentence) if word not in stop_words]\n",
    "        similarity_distance = WMD_model.wmdistance(sent1, sent2)\n",
    "        list_distances.append(similarity_distance)\n",
    "        list_sentence_index.append(index)\n",
    "        index+=1\n",
    "    WMD_Dataframe = pd.DataFrame({'Sentence': sentences, 'Sentence_Index': list_sentence_index, 'WMD_Score': list_distances}).sort_values(by=['WMD_Score'],ascending=True) \n",
    "    Top8_sentences = ' '.join([sent for sent in WMD_Dataframe[0:8].Sentence])\n",
    "   \n",
    "    return Top8_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def query_to_paragraph(query):\n",
    "    document_name = query_to_document(query)\n",
    "    document_context = dictionary_document_context[document_name]\n",
    "    paragraph = document_to_paragraph(query=query, document=document_context)\n",
    "    return paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query and Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a query: when did spectre movie release?\n"
     ]
    }
   ],
   "source": [
    "query = input(\"Enter a query: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "query = ''.join(e for e in query if (e.isalnum() or e==' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "paragraph = query_to_paragraph(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "It was later revealed with the film's release that he is Ernst Stavro Blofeld. Sam Mendes has stated he will not return to direct the next 007 film. Satisfied with the quality, the demo was used in the final release. Forbes' Scott Mendelson also heavily criticised the film, denouncing Spectre as \"the worst 007 movie in 30 years\". [N 3] A second remake, entitled Warhead 2000 A.D., was planned for production and release in the 1990s before being abandoned. Only eight of those 10 were used for the film, however; the remaining two were used for promotional work. In the United States and Canada, the film opened on 6 November 2015, and in its opening weekend, was originally projected to gross $70–75 million from 3,927 screens, the widest release for a Bond film. Mendes revealed that the final film would have more than one hundred minutes of music.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\"+ paragraph+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
